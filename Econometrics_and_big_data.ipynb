{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQo1nOMpIPj1uF8agmGvIW"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afeJqu87ds2_"
      },
      "outputs": [],
      "source": [
        "pip install ISLP"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "from ISLP import load_data\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor"
      ],
      "metadata": {
        "id": "5Guxho87epaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Boston dataset\n",
        "Boston = load_data(\"Boston\")\n",
        "X = Boston.drop('medv', axis=1).values # save all the values apart from medv in x\n",
        "y = Boston['medv'].values # save all the values of medv in y\n",
        "# Split the data into 30% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "KCi9aqtad1q2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "My dog is not sick"
      ],
      "metadata": {
        "id": "qALJaM82BWSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the initial regression tree\n",
        "initial_tree = DecisionTreeRegressor(random_state=42)\n",
        "initial_tree.fit(X_train, y_train)\n",
        "\n",
        "# Get the cost complexity pruning path\n",
        "# Start with the full tree, find the change in MSE for every added branch,\n",
        "# prune the branches starting with the smallest increase in MSE. Each time a branch is\n",
        "# cut an assosiated alpha values is saved in ccp_alphas. The tree is then reuvalated for the best cut.\n",
        "path = initial_tree.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
        "alpha_results = [] # create an empty list, this will be used to store the CCP alpha values\n",
        "\n",
        "# Use CV with every ccp_alpha\n",
        "for ccp_alpha in ccp_alphas:\n",
        "    tree = DecisionTreeRegressor(random_state=42, ccp_alpha=ccp_alpha) # specific tree with related alpha value\n",
        "    scores = cross_val_score(tree, X_train, y_train, cv=5, scoring='neg_mean_squared_error') # perfrom cv\n",
        "    average_mse = -scores.mean()  # convert negative and take the mean\n",
        "    alpha_results.append({\"ccp_alpha\": ccp_alpha, \"MSE\": average_mse}) # save it in the list alpha_results\n",
        "\n",
        "cv_results_df = pd.DataFrame(alpha_results) # save it in a df for easier viewing\n",
        "optimal_alpha = cv_results_df.loc[cv_results_df['MSE'].idxmin(), 'ccp_alpha'] # find the smallest MSE\n",
        "\n",
        "# Using the optimal alpha fit the best reg tree\n",
        "optimal_tree = DecisionTreeRegressor(random_state=42, ccp_alpha=optimal_alpha)\n",
        "optimal_tree.fit(X_train, y_train)\n",
        "predictions = optimal_tree.predict(X_test)\n",
        "optimal_test_mse = mean_squared_error(y_test, predictions)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Optimal CCP Alpha: {optimal_alpha}\")\n",
        "print(f\"Optimal MSE on test data: {optimal_test_mse}\")\n",
        "pd.set_option('display.max_rows', None)\n",
        "#cv_results_df - this is the whole dataframe containing all the alpha with the correpsonding MSE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lE7gYo6gd33C",
        "outputId": "fa9286fd-4d7d-42da-e491-a2569b113a1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal CCP Alpha: 0.18176217107303233\n",
            "Optimal MSE on test data: 10.039659929938216\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BAGGING\n",
        "bagging_model = BaggingRegressor(estimator=DecisionTreeRegressor(), random_state=42)\n",
        "\n",
        "# Define the parameter to search through, removed some options from final to allow faster compuation\n",
        "param_grid = {\n",
        "    'n_estimators': [140, 150],  # number of trees\n",
        "    'max_samples': [0.9, 1.0], # sample size used to rain each tree on\n",
        "    'max_features': [0.9], # features to consider for each split\n",
        "    'estimator__max_depth': [15, 16, 17], # depth of each tree\n",
        "    'estimator__min_samples_split': [2], # min number of samples for each split\n",
        "    'estimator__min_samples_leaf': [1] # min observations each leaf must contain\n",
        "}\n",
        "\n",
        "# Search through all the options specified in the params_grid with cv\n",
        "grid_search = GridSearchCV(estimator=bagging_model, param_grid=param_grid,\n",
        "                           cv=5, n_jobs=-1, scoring='neg_mean_squared_error', verbose=2)\n",
        "grid_search.fit(X_train, y_train) # fit the gridsearch to the data\n",
        "print(f\"Best parameters: {grid_search.best_params_}\") # print optimal paramters accroding to gridsearch\n",
        "\n",
        "# Make predictions using the optimal model from grid search\n",
        "best_model = grid_search.best_estimator_\n",
        "bagging_predictions = best_model.predict(X_test)\n",
        "\n",
        "# Calculate the MSE on the test data\n",
        "bagging_test_mse = mean_squared_error(y_test, bagging_predictions)\n",
        "print(f\"GridSearchCV Bagging MSE on test data: {bagging_test_mse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGsxtGj4fX1_",
        "outputId": "a7b73015-2c47-4967-8cf5-d07629899bdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
            "Best parameters: {'estimator__max_depth': 17, 'estimator__min_samples_leaf': 1, 'estimator__min_samples_split': 2, 'max_features': 0.9, 'max_samples': 0.9, 'n_estimators': 150}\n",
            "GridSearchCV Bagging MSE on test data: 10.676318108499382\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RANDOM FOREST\n",
        "random_forest_model = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Define the parameter grid - you may adjust this based on your computational budget and needs- removed some options from final to allow faster compuation\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 110, 120],  # number of trees in forest\n",
        "    'max_depth': [15],  # max depth of tree\n",
        "    'min_samples_split': [2, 3],  # min sampels for split\n",
        "    'min_samples_leaf': [1, 2],  # min observations for each leaf\n",
        "    'max_features': ['sqrt'],  # features to condiser for every split ['auto', 'sqrt', 'log2']\n",
        "    'max_samples': [1.0]  # the number of samples to draw from X to train each base estimator\n",
        "}\n",
        "\n",
        "# Search through all the options specified in the params_grid with cv\n",
        "grid_search_rf = GridSearchCV(estimator=random_forest_model, param_grid=param_grid,\n",
        "                              cv=5, n_jobs=-1, scoring='neg_mean_squared_error', verbose=2)\n",
        "# fit the model with the parameters from gridsearch\n",
        "grid_search_rf.fit(X_train, y_train)\n",
        "print(f\"Best parameters: {grid_search_rf.best_params_}\")\n",
        "\n",
        "# Make predictions using the best model from grid search\n",
        "best_model_rf = grid_search_rf.best_estimator_\n",
        "rf_predictions = best_model_rf.predict(X_test)\n",
        "\n",
        "# Calculate the MSE with test data\n",
        "rf_test_mse = mean_squared_error(y_test, rf_predictions)\n",
        "print(f\"GridSearchCV Random Forest MSE on test data: {rf_test_mse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrY5XPst36wZ",
        "outputId": "46f97b55-49bd-4d98-ab9a-39a52c9420eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
            "Best parameters: {'max_depth': 15, 'max_features': 'sqrt', 'max_samples': 1.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 110}\n",
            "GridSearchCV Random Forest MSE on test data: 9.853333825932712\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BOOSTING\n",
        "# Define the parameter grid - removed some options from final to allow faster compuation\n",
        "param_grid = {\n",
        "    'n_estimators': [180, 190, 200], # number of trees\n",
        "    'learning_rate': [0.05, 0.1, 0.15],  # rate of learning\n",
        "    'max_depth': [3],  # depth of each tree [3, 5, 7, 9, 11]\n",
        "    'min_samples_split': [8],  # min number of samples required to split a node [2, 4, 6, 8, 10]\n",
        "    'min_samples_leaf': [1, 2]  # min number of samples required to be at a leaf node\n",
        "}\n",
        "\n",
        "# build a Gradient Boosting model\n",
        "gradient_boosting_model = GradientBoostingRegressor(random_state=42)\n",
        "# search through params_grid\n",
        "grid_search_gb = GridSearchCV(estimator=gradient_boosting_model, param_grid=param_grid,\n",
        "                              cv=5, n_jobs=-1, scoring='neg_mean_squared_error', verbose=2)\n",
        "# Fit with best params\n",
        "grid_search_gb.fit(X_train, y_train)\n",
        "print(f\"Best parameters: {grid_search_gb.best_params_}\")\n",
        "\n",
        "# predictions with best parameters\n",
        "best_gb_model = grid_search_gb.best_estimator_\n",
        "gb_predictions = best_gb_model.predict(X_test)\n",
        "\n",
        "# find the MSE\n",
        "gb_test_mse = mean_squared_error(y_test, gb_predictions)\n",
        "print(f\"GridSearchCV Gradient Boosting MSE on test data: {gb_test_mse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZC7JU2r4jac",
        "outputId": "454e1de9-4989-4a44-d911-c45ec2f9fcae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "Best parameters: {'learning_rate': 0.1, 'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 8, 'n_estimators': 190}\n",
            "GridSearchCV Gradient Boosting MSE on test data: 7.907888797423849\n"
          ]
        }
      ]
    }
  ]
}